{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6275253",
   "metadata": {},
   "source": [
    "# Task 1 Evaluation\n",
    "\n",
    "This notebook contains the evaluation for Task 1 of the TREC Fair Ranking track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c102b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by loading necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a056319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import binpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e4899",
   "metadata": {},
   "source": [
    "Set up progress bar and logging support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28734344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas(leave=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86555293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stderr)\n",
    "log = logging.getLogger('task1-eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b7296",
   "metadata": {},
   "source": [
    "Import metric code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc40035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "from trecdata import scan_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf20fb",
   "metadata": {},
   "source": [
    "And finally import the metric itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84558415",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = binpickle.load('task1-eval-metric.bpk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c96e2",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa5969b",
   "metadata": {},
   "source": [
    "Let's load the runs now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e9219",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = pd.DataFrame.from_records(row for (task, rows) in scan_runs() if task == 1 for row in rows)\n",
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb916f89",
   "metadata": {},
   "source": [
    "Since we only have annotations for the first 20 for each run, limit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9509801",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = runs[runs['rank'] <= 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54b23b",
   "metadata": {},
   "source": [
    "## Computing Metrics\n",
    "\n",
    "We are now ready to compute the metric for each (system,topic) pair.  Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc702eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_awrf = runs.groupby(['run_name', 'topic_id'])['page_id'].progress_apply(metric)\n",
    "rank_awrf = rank_awrf.unstack()\n",
    "rank_awrf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbf43de",
   "metadata": {},
   "source": [
    "Now let's average by runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scores = rank_awrf.groupby('run_name').mean()\n",
    "run_scores.sort_values('Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c28120f",
   "metadata": {},
   "source": [
    "## Analyzing Scores\n",
    "\n",
    "What is the distribution of scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a789c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(x='Score', data=run_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='nDCG', y='AWRF', data=run_scores)\n",
    "sns.rugplot(x='nDCG', y='AWRF', data=run_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c97d4d",
   "metadata": {},
   "source": [
    "## Per-Topic Stats\n",
    "\n",
    "We need to return per-topic stats to each participant, at least for the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15b0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stats = rank_awrf.groupby('topic_id').agg(['mean', 'median', 'min', 'max'])\n",
    "topic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfdc85",
   "metadata": {},
   "source": [
    "Make final score analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_range = topic_stats.loc[:, 'Score']\n",
    "topic_range = topic_range.drop(columns=['mean'])\n",
    "topic_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16988437",
   "metadata": {},
   "source": [
    "And now we combine scores with these results to return to participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d542791",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dir = Path('results')\n",
    "for system, runs in rank_awrf.groupby('run_name'):\n",
    "    aug = runs.join(topic_range).reset_index().drop(columns=['run_name'])\n",
    "    fn = ret_dir / f'{system}.tsv'\n",
    "    log.info('writing %s', fn)\n",
    "    aug.to_csv(fn, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bc287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
