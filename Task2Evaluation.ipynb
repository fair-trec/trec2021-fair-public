{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8d7b36",
   "metadata": {},
   "source": [
    "# Task 2 Evaluation\n",
    "\n",
    "This notebook contains the evaluation for Task 1 of the TREC Fair Ranking track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd60a26",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We begin by loading necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a17561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gzip\n",
    "import binpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d184df5",
   "metadata": {},
   "source": [
    "Set up progress bar and logging support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas(leave=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, logging\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stderr)\n",
    "log = logging.getLogger('task1-eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98defdf6",
   "metadata": {},
   "source": [
    "Import metric code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ebf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "from trecdata import scan_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474bb6c9",
   "metadata": {},
   "source": [
    "And finally import the metric itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac7aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = binpickle.load('task2-eval-metric.bpk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1efc981",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20173737",
   "metadata": {},
   "source": [
    "Let's load the runs now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = pd.DataFrame.from_records(row for (task, rows) in scan_runs() if task == 2 for row in rows)\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f12efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3b38e",
   "metadata": {},
   "source": [
    "We also need to load our topic eval data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c6316",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pd.read_json('data/eval-topics.json.gz', lines=True)\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e861d729",
   "metadata": {},
   "source": [
    "Tier 2 is the top 5 docs of the first 25 rankings.  Further, we didn't complete Tier 2 for all topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7678b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_topics = topics.loc[topics['max_tier'] >= 2, 'id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1126dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_top5 = runs['rank'] <= 5\n",
    "r_first25 = runs['seq_no'] <= 25\n",
    "r_done = runs['topic_id'].isin(t2_topics)\n",
    "runs = runs[r_done & r_top5 & r_first25]\n",
    "runs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc9def",
   "metadata": {},
   "source": [
    "## Computing Metrics\n",
    "\n",
    "We are now ready to compute the metric for each (system,topic) pair.  Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd845d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_exp = runs.groupby(['run_name', 'topic_id']).progress_apply(metric)\n",
    "# rank_exp = rank_awrf.unstack()\n",
    "rank_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e18b4",
   "metadata": {},
   "source": [
    "Now let's average by runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32e4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scores = rank_exp.groupby('run_name').mean()\n",
    "run_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e87fb8",
   "metadata": {},
   "source": [
    "## Analyzing Scores\n",
    "\n",
    "What is the distribution of scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(x='EE-L', data=run_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b09ba-17da-4365-b993-5fd1f7a34f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scores.sort_values('EE-L', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='EE-D', y='EE-R', data=run_scores)\n",
    "sns.rugplot(x='EE-D', y='EE-R', data=run_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d838b4c",
   "metadata": {},
   "source": [
    "## Per-Topic Stats\n",
    "\n",
    "We need to return per-topic stats to each participant, at least for the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stats = rank_exp.groupby('topic_id').agg(['mean', 'median', 'min', 'max'])\n",
    "topic_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc0ad5",
   "metadata": {},
   "source": [
    "Make final score analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c4ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_range = topic_stats.loc[:, 'EE-L']\n",
    "topic_range = topic_range.drop(columns=['mean'])\n",
    "topic_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8891ec",
   "metadata": {},
   "source": [
    "And now we combine scores with these results to return to participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3726e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dir = Path('results')\n",
    "for system, runs in rank_exp.groupby('run_name'):\n",
    "    aug = runs.join(topic_range).reset_index().drop(columns=['run_name'])\n",
    "    fn = ret_dir / f'{system}.tsv'\n",
    "    log.info('writing %s', fn)\n",
    "    aug.to_csv(fn, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce13d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
